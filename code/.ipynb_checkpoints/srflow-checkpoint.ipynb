{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to environment variables\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import basename\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "import logging\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import options.options as option\n",
    "from utils import util\n",
    "from data import create_dataloader, create_dataset\n",
    "from models import create_model\n",
    "from utils.timer import Timer, TickTock\n",
    "from utils.util import get_resume_paths\n",
    "\n",
    "\n",
    "def getEnv(name): import os; return True if name in os.environ.keys() else False\n",
    "\n",
    "\n",
    "def init_dist(backend='nccl', **kwargs):\n",
    "    ''' initialization for distributed training'''\n",
    "    # if mp.get_start_method(allow_none=True) is None:\n",
    "    if mp.get_start_method(allow_none=True) != 'spawn':\n",
    "        mp.set_start_method('spawn')\n",
    "    rank = int(os.environ['RANK'])\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    torch.cuda.set_deviceDistIterSampler(rank % num_gpus)\n",
    "    dist.init_process_group(backend=backend, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sep/minjun/anaconda3/envs/srflow/lib/python3.7/site-packages/ipykernel_launcher.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('./confs/SRFlow_DF2K_4X.yml', 'r') as f:\n",
    "    opt = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabled distributed training.\n"
     ]
    }
   ],
   "source": [
    "#### distributed training settings\n",
    "opt['dist'] = False\n",
    "rank = -1\n",
    "print('Disabled distributed training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabled distributed training.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fc081d3bf288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mresume_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdevice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         resume_state = torch.load(resume_state_path,\n\u001b[1;32m     16\u001b[0m                                   map_location=lambda storage, loc: storage.cuda(device_id))\n",
      "\u001b[0;32m/sep/minjun/anaconda3/envs/srflow/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sep/minjun/anaconda3/envs/srflow/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "#### distributed training settings\n",
    "opt['dist'] = False\n",
    "rank = -1\n",
    "print('Disabled distributed training.')\n",
    "\n",
    "#### loading resume state if exists\n",
    "if opt['path'].get('resume_state', None):\n",
    "    resume_state_path, _ = get_resume_paths(opt)\n",
    "\n",
    "    # distributed resuming: all load into default GPU\n",
    "    if resume_state_path is None:\n",
    "        resume_state = None\n",
    "    else:\n",
    "        device_id = torch.cuda.current_device()\n",
    "        resume_state = torch.load(resume_state_path,\n",
    "                                  map_location=lambda storage, loc: storage.cuda(device_id))\n",
    "        option.check_resume(opt, resume_state['iter'])  # check resume options\n",
    "else:\n",
    "    resume_state = None\n",
    "\n",
    "#### mkdir and loggers\n",
    "if rank <= 0:  # normal training (rank -1) OR distributed training (rank 0)\n",
    "    if resume_state is None:\n",
    "        util.mkdir_and_rename(\n",
    "            opt['path']['experiments_root'])  # rename experiment folder if exists\n",
    "        util.mkdirs((path for key, path in opt['path'].items() if not key == 'experiments_root'\n",
    "                     and 'pretrain_model' not in key and 'resume' not in key))\n",
    "\n",
    "    # config loggers. Before it, the log will not work\n",
    "    util.setup_logger('base', opt['path']['log'], 'train_' + opt['name'], level=logging.INFO,\n",
    "                      screen=True, tofile=True)\n",
    "    util.setup_logger('val', opt['path']['log'], 'val_' + opt['name'], level=logging.INFO,\n",
    "                      screen=True, tofile=True)\n",
    "    logger = logging.getLogger('base')\n",
    "    logger.info(option.dict2str(opt))\n",
    "\n",
    "    # tensorboard logger\n",
    "    if opt.get('use_tb_logger', False) and 'debug' not in opt['name']:\n",
    "        version = float(torch.__version__[0:3])\n",
    "        if version >= 1.1:  # PyTorch 1.1\n",
    "            from torch.utils.tensorboard import SummaryWriter\n",
    "        else:\n",
    "            logger.info(\n",
    "                'You are using PyTorch {}. Tensorboard will use [tensorboardX]'.format(version))\n",
    "            from tensorboardX import SummaryWriter\n",
    "        conf_name = basename(args.opt).replace(\".yml\", \"\")\n",
    "        exp_dir = opt['path']['experiments_root']\n",
    "        log_dir_train = os.path.join(exp_dir, 'tb', conf_name, 'train')\n",
    "        log_dir_valid = os.path.join(exp_dir, 'tb', conf_name, 'valid')\n",
    "        tb_logger_train = SummaryWriter(log_dir=log_dir_train)\n",
    "        tb_logger_valid = SummaryWriter(log_dir=log_dir_valid)\n",
    "else:\n",
    "    util.setup_logger('base', opt['path']['log'], 'train', level=logging.INFO, screen=True)\n",
    "    logger = logging.getLogger('base')\n",
    "\n",
    "# convert to NoneDict, which returns None for missing keys\n",
    "opt = option.dict_to_nonedict(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b70956cfeb78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_opt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datasets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dataset created'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "for phase, dataset_opt in opt['datasets'].items():\n",
    "    if phase == 'train':\n",
    "        print(dataset_opt)\n",
    "        train_set = create_dataset(dataset_opt)\n",
    "        print('Dataset created')\n",
    "        train_size = int(math.ceil(len(train_set) / dataset_opt['batch_size']))\n",
    "        total_iters = int(opt['train']['niter'])\n",
    "        total_epochs = int(math.ceil(total_iters / train_size))\n",
    "        train_sampler = None\n",
    "        train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)\n",
    "#         if rank <= 0:\n",
    "#             logger.info('Number of train images: {:,d}, iters: {:,d}'.format(\n",
    "#                 len(train_set), train_size))\n",
    "#             logger.info('Total epochs needed: {:d} for iters {:,d}'.format("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5a7bb51f169d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m#if resume_state is None else resume_state['iter']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "current_step = 0 #if resume_state is None else resume_state['iter']\n",
    "model = create_model(opt, current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
